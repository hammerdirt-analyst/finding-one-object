{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sys, file and nav packages:\n",
    "import datetime as dt\n",
    "import warnings\n",
    "\n",
    "# math packages:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import norm\n",
    "import pymc3 as pm\n",
    "import empiricaldist\n",
    "from empiricaldist import Pmf\n",
    "import arviz as az\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib import colors\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# images and display\n",
    "# import base64, io, IPython\n",
    "from PIL import Image as PILImage\n",
    "from IPython.display import Markdown as md\n",
    "from IPython.display import display\n",
    "\n",
    "# set some parameters:\n",
    "# today = dt.datetime.now().date().strftime(\"%Y-%m-%d\")\n",
    "start_date = '2015-11-01'\n",
    "end_date ='2021-11-01'\n",
    "\n",
    "dfBeaches = pd.read_csv(\"resources/beaches_with_land_use_rates.csv\")\n",
    "dfDims = pd.read_csv(\"resources/corrected_dims.csv\")\n",
    "\n",
    "# set the index of the beach data to location slug\n",
    "dfBeaches.set_index('slug', inplace=True)\n",
    "\n",
    "# map location slug to the proper city name\n",
    "city_map = dfBeaches['city']\n",
    "\n",
    "# map locations to feature names\n",
    "location_wname_key = dfBeaches.water_name_slug\n",
    "\n",
    "# all data since 2015\n",
    "x = pd.read_csv(\"resources/checked_before_agg_sdata_eos_2020_21.csv\")\n",
    "\n",
    "# xl = x[x.water_name_slug == \"lac-leman\"].copy()\n",
    "\n",
    "ldb = x[x.water_name_slug == \"lac-leman\"].copy()\n",
    "\n",
    "# assign loc_date\n",
    "ldb['loc_date'] = list(zip(ldb.location, ldb['date']))\n",
    "\n",
    "# date to datetime\n",
    "ldb['date'] = pd.to_datetime(ldb['date'], format=\"%Y-%m-%d\")\n",
    "ldb = ldb[ldb[\"date\"]<=\"2021-11-01\"]\n",
    "ldb[\"year\"] = ldb[\"date\"].dt.year\n",
    "\n",
    "# agg levels\n",
    "samp_loc = ['loc_date', 'location']\n",
    "ldate = ['loc_date']\n",
    "city = ['Genève']\n",
    "\n",
    "# agg columns\n",
    "qandp = {'quantity':'sum', 'pcs_m':'sum'}\n",
    "qandnsamps = {'quantity': 'sum', 'loc_date':'nunique'}\n",
    "\n",
    "# agg codes and groups\n",
    "code = ['G144', 'G96']\n",
    "# groupname = ['waste water']\n",
    "\n",
    "sg = ldb.city.isin(city)\n",
    "# fhw = (ldb.groupname.isin(groupname))\n",
    "fhg = ldb.code.isin(code)\n",
    "gzero = ldb.quantity > 0\n",
    "\n",
    "# define the sampling periods\n",
    "\n",
    "# year one\n",
    "y_o_s = \"2015-11-15\"\n",
    "y_o_e = \"2017-03-31\"\n",
    "\n",
    "# year two\n",
    "y_t_s = \"2017-04-01\"\n",
    "y_t_e = \"2018-05-01\"\n",
    "\n",
    "# year three\n",
    "y_th_s = \"2021-11-01\"\n",
    "\n",
    "# map the periods to the data\n",
    "y_one = ldb['date'] <= y_o_e\n",
    "y_two = (ldb['date'] >= y_t_s) & (ldb['date'] <= y_t_e)\n",
    "y_thre = (ldb['date'] > y_t_e) &(ldb['date'] <= y_th_s)\n",
    "\n",
    "# make a seperate df for each\n",
    "year_one = ldb[y_one].copy()\n",
    "year_two = ldb[y_two].copy()\n",
    "year_three = ldb[y_thre].copy()\n",
    "\n",
    "# store the dfs in an array\n",
    "dfs =  [year_one, year_two, year_three]\n",
    "\n",
    "# summarize all instances at all locations\n",
    "# the total objects per location, number of samples, number of target objects, number of samples with target objects\n",
    "# adlt = ldb.groupby(['city','location']).agg(qandnsamps)\n",
    "\n",
    "# adlt.rename(columns={'loc_date':'n samples', 'quantity':'qd'}, inplace=True)\n",
    "# foundfhg =ldb[fhg&gzero].groupby(['city','location']).agg(qandnsamps)\n",
    "\n",
    "# fhg_summary = pd.concat([adlt,foundfhg], axis=1)\n",
    "# fhg_summary.fillna(0, inplace=True)\n",
    "# fhg_summary.rename(columns={'quantity':'q_fhg', 'loc_date':'s_pos'}, inplace=True)\n",
    "# fhg_summary.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_a_summary_table(ax, data,colLabels, a_color=\"black\", font_size=12, s_et_bottom_row=True, coded_labels=False, codes=[]):\n",
    "    \n",
    "    \"\"\"Formats matplotlib table object.\n",
    "\n",
    "    Args:\n",
    "    ax: object: matplotlib table object\n",
    "    data: array: the 2d array used to generate the table object\n",
    "    cols_to_use: array: the list of column names\n",
    "    a_color: str: matplotlib named color, face and edgecolor of table cells\n",
    "    font_size: int: the font size for the table cells\n",
    "    s_et_bottom_row: bool: whether or not to draw bottom line on the last row\n",
    "\n",
    "    Returns:\n",
    "    The table object formatted.\n",
    "    \"\"\"\n",
    "\n",
    "    ax.auto_set_font_size(False)\n",
    "    the_cells = ax.get_celld()\n",
    "\n",
    "    line_color = mcolors.to_rgba(\"black\")\n",
    "    banded_color = (*line_color[:-1], 0.1)\n",
    "\n",
    "    # the different areas of formatting\n",
    "    top_row = [(0, i) for i in np.arange(len(colLabels))]\n",
    "    bottom_row = [(len(data), i) for i in np.arange(len(colLabels))]\n",
    "    data_rows = [x for x in list(the_cells.keys()) if x not in top_row]\n",
    "    \n",
    "    if coded_labels:\n",
    "        for i, a_cell in enumerate(top_row):\n",
    "            ax[a_cell].get_text().set_text(\"\")\n",
    "            \n",
    "            cell_color = mcolors.to_rgba(codes[i])\n",
    "            c_color = (*cell_color[:-1], 0.6)\n",
    "            ax[a_cell].set_facecolor(c_color)\n",
    "            ax[a_cell].set_linewidth = 1\n",
    "            ax[a_cell].set_height( .4/ (len(data)))\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        for i, a_cell in enumerate(top_row):\n",
    "            ax[a_cell].visible_edges = \"B\"\n",
    "            ax[a_cell].set_text_props(**{\"fontsize\": font_size})\n",
    "            ax[a_cell].set_edgecolor(\"black\")\n",
    "            \n",
    "            ax[a_cell].PAD = .2\n",
    "            ax[a_cell].set_linewidth = 1\n",
    "            ax[a_cell].set_height(.5 / (len(data)))\n",
    "\n",
    "    for a_cell in data_rows:\n",
    "        ax[a_cell].set_height(.5 / (len(data)))\n",
    "        ax[a_cell].visible_edges = \"BT\"\n",
    "        ax[a_cell].set_text_props(**{\"fontsize\": font_size})\n",
    "        ax[a_cell].set_edgecolor(banded_color)\n",
    "        ax[a_cell]._text.set_horizontalalignment(\"center\")\n",
    "        ax[a_cell].set_linewidth = .1\n",
    "\n",
    "    if s_et_bottom_row is True:\n",
    "        \n",
    "        for a_cell in bottom_row:\n",
    "            ax[a_cell].visible_edges = \"B\"\n",
    "            ax[a_cell].set_edgecolor(line_color)\n",
    "            ax[a_cell].set_linewidth = 1\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "\n",
    "def a_simple_formatted_table(ax,data,colLabels=[], a_color=\"black\", colWidths=[], bbox=[], coded_labels=False, codes=[],**kwargs):\n",
    "    \"\"\"Makes a table with rows from a matplotlib axes object and a 2d array. Header row is\n",
    "    spererated from table body by a thicker black line. \n",
    "    \n",
    "    :param ax: An axes\n",
    "    :type ax: matplotlib axes\n",
    "    :param data: An array of the table values not including column names or labels\n",
    "    :type data: array\n",
    "    :param colLabels: The labels for the data table columns\n",
    "    :type colLabels: array\n",
    "    :param a_color: The color of the cell borders\n",
    "    :type a_color: str\n",
    "    :param colWidths: The width of each column in fractions of 1\n",
    "    :type colWdiths: array, x < 1\n",
    "    :param bbox: The location of the table in figure space\n",
    "    :type bbox: array\n",
    "    :return: A table on the provided axis\n",
    "    :rtype: matplotlib.axes\n",
    "    \n",
    "    \"\"\"\n",
    "    a = ax.table(data,  colLabels=colLabels, colWidths=colWidths, bbox=bbox, loc=\"lower center\", **kwargs)\n",
    "    t = make_a_summary_table(a, data, colLabels, a_color=a_color, font_size=12, s_et_bottom_row=False, coded_labels=coded_labels, codes=codes)\n",
    "    return t\n",
    "def remove_spines(ax):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    \n",
    "    return ax\n",
    "def remove_ticks(ax):\n",
    "    ax.get_xaxis().set_ticks([])\n",
    "    ax.get_yaxis().set_ticks([])\n",
    "    \n",
    "    return ax   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## INTRODUCTION \n",
    "\n",
    "According to a Brief History of Marine Litter, the first scientifically recorded interaction between marine organisms and persistent litter was in 1969 {cite}`briefhistory`. In 1972 the International Journal of Environmental Studies observed that most of the trash on isolated stretches of ocean shoreline was a result of sea‐borne waste. The waste is primarily a by‐product of international commerce and not the behavior of the casual visitor {cite}`scottdebunksource`.  This document picks up the trail 50 years later and several hundred miles upstream.  While the harmful effects of plastic are (apparently) debatable, its occurrence on our beaches and in our water is undeniable.  Volunteers, applying common sense protocols put in place over the past twenty years, have created a very accurate picture of the occurrence of plastics at sea and on inland lakes and rivers.  \n",
    "\n",
    "The United Nations published a first international guide to collecting ocean beach litter in 2008 {cite}`unepseas`. This publication was followed by another guide developed by OSPAR {cite}`osparguidelines` in 2010 and then, in 2013, the EU published the Guidance on Monitoring Marine Litter in European Seas {cite}`mlwguidance`. Riverine Litter Monitoring - Options and Recommendations was published later, in 2016, as evidence was mounting that rivers are major sources of marine litter.  {cite}`riverinemonitor` As a result, thousands of observations have been collected following a very similar protocol {cite}`mlwdata` {cite}`ospardata`. These data are collected by different organizations, mostly volunteer, throughout the European continent. Each observation is a categorical list of objects and their respective quantities within a defined length of shoreline, the EU standard is 100 meters. However, when the baseline values were defined, all samples within a beach length greater than 10 meters were considered {cite}`mlwguidance` {cite}`osparguidelines`. The same protocol has been in place in Switzerland since November 2015, targeting regional lakes and rivers. {cite}`iqaasl` \n",
    "\n",
    "The data collected by volunteers were considered fit for the purpose of establishing beach-litter threshold values by the Marine Litter Technical Group of the EU. However, the lack of quantitative research on the harmful effects of beach litter, specifically the dose-effect relationship between plastic and ecological harm, precludes the establishment of threshold values based on this metric. Therefore, threshold values and baselines needed to be adopted according to the precautionary principle to limit exposure. Threshold values could not be based on the socio-economic effects of beach litter either because the negative effects have proven difficult to define quantitatively.  {cite}`threshholdeu`. In practice, EU threshold values are determined by using the 15th percentile (20 p/100m) from the combined data set of 2015-2016 beach litter surveys within the EU. {cite}`threshholdeu`  Thus, this threshold can be interpreted as a ratio of the given the survey data greater than 20 p/100m or 85%. \n",
    "\n",
    " Considering that the median value was 133 p/100 m in the 2015-2016 beach litter surveys, it means that regional administrations will need to allocate resources to meet the threshold value if they want to achieve good environmental status under the Marine Strategy Framework Directive {cite}`goodenvironmentalstanding`. At the same time, determining which are the most effective solutions will require that stakeholders conduct intermediate assessments based on benchmarks to evaluate progress and compare probable outcomes. This requires having adequate statistical tools. \n",
    "\n",
    "Additionally, the potential applications for beach litter data go beyond establishing threshold values. Developing Life Cycle Assessments is an excellent example. The Life Cycle Initiative (LCI) is a public-private partnership that includes France, Switzerland, Germany and the EU with a stated goal of advancing the understanding of life cycle thinking by private and public decision makers. In partnership with Plastics Europe, LCI has been attempting \"to integrate potential environmental impacts of marine litter, especially plastic, in Life Cycle Assessment (LCA) results\".  {cite}`marilca` {cite}`lci`.The consequences of plastic leaked into the environment are not accounted for in the current practice of LCA {cite}`woods2021107918`. This could be in part for the same reasons that the EU adopted the precautionary principle, as opposed to dose-effect relationships when developing threshold values.   \n",
    "\n",
    "Nevertheless, recent attempts have been made to develop a Marine Litter Indicator (MLI) for LCAs. For instance, based on the commonsense hypothesis that there is a relationship between the number of objects produced, value of those objects once the product is consumed, and the likelihood that they will be littered, it was concluded that returnable PET bottles or returnable glass bottles would have the lowest MLI, all things considered {cite}`plasticorglass`. A very similar method was used when proposing an MLI for plastic carrier bags. {cite}` civancikuslu2019621` \n",
    "\n",
    "If the reason for adopting a threshold value is based on the precautionary principle, then, ultimately, the threshold value is an attempt to limit exposure. For the LCA it appears that the lifecycle of the product ends when the original product is no longer identifiable {cite}`plasticorglass`, ie.. exposure ends when the object/product is no longer recognizable. Beach litter surveys account for both cases (i) Identifiable objects and (ii) fragmented plastics of different sizes.  Beach litter survey results are not included in the calculation of the proposed MLI. {cite}`plasticorglass` {cite}`civancikuslu2019621` \n",
    "\n",
    "In both cases, EU thresholds and LCAs, are attempts at reducing the probability of the event “an object was found on the beach today” but none answers the general question “How likely is an object to be found”.   This can be done by recalling that probability is a ratio not a frequency and applying Bayes theorem to the survey results for objects or groups of objects {cite}`Jaynes`. The development and application of the method is illustrated through its application to a particular case: object, feminine hygiene products (FHP) and system, beaches in Geneva, Switzerland. Both are briefly introduced below. \n",
    "\n",
    "Clean up events and awareness campaigns have played a major role in identifying the principal components of beach litter and the presence of plastics in the environment. One such campaign is End Period Plastics in the UK who is working to eliminate plastics in feminine hygiene products (FHP). The campaign includes meetings throughout Europe with manufacturers, suppliers and vendors of plastic FHP. In preparation for a meeting with a producer of FHP in Geneva, the team from End Period Plastics was interested in knowing the incidence of these objects in Lake Geneva {cite}`endperiodplastics`. Their request is at the origin of this study. Tampon applicators and tampons are part of a group of specific items that are found on beaches and that most likely originate from toilet flushing or a wastewater treatment facility, cotton-swabs are also part of that group.  {cite}`obriain2020116021` {cite}`padbackingstrips` {cite}`increasingplastics`  \n",
    "\n",
    "Beach litter monitoring on Lake Geneva started in November 2015 using the MSFD method. Data has been collected at irregular intervals since then. Geneva beaches (xxx) have been sampled intermittently since the start, in contrast to other beaches such as those in other parts of the lake such as in Saint Sulpice (xxx) which has fewer samples but has samples between October and November every year. The Rhône outflow into the Mediterranean Sea is also monitored using the same protocol as Lake Geneva thus using the values from different locations in the same river basin to assess probable values at any location in that river basin depending on the data.  Bayes theorem is used as a method to determine the location in Geneva beaches where the probability of finding an FHP is highest by comparing the probability of finding an FHP and covariates at different temporal and geographical bounds. A litter survey was conducted with End Period Plastics staff and hammerdirt surveyors to illustrate the abundance of FHP on the lake. Because of time and transportation constraints, this litter survey had to be conducted on a site with the following criteria: (i) access with public transport, (ii) completed in less than three hours, and (iii) preferably a location where there was a chance to find this specific item quickly. \n",
    "\n",
    "## DATA AND METHODS \n",
    "\n",
    "### Study sites \n",
    "\n",
    "Lake Geneva is a perialpine lake at an altitude of 372 m above mean sea level located between France and Switzerland. It has an elongated shape, a surface area of 580 km2, and maximum length and width of 72.3 and 14 km, respectively. The average water residence time is 11.3 years. The lake is subdivided into two sub-basins: the Grand Lac (large lake) (309 m deep) and the Petit Lac (small lake) (medium-sized basin with a volume of 3 km3 and a maximum depth of about 70 m.  Lake Geneva is fed by a large number of rivers and streams but most of the water enters through the Rhône River. The permanent population (2011) in its watershed is: 1,083,431 (France: 142,229, Switzerland: 941,202) but also hosts a large tourism population. There are 171 wastewater treatment plants (population equivalent: 3,009,830). {cite}`cipel2019` \n",
    "\n",
    "Geneva beaches (baby plage, baby plage II, Jardin botanique, rocky plage, villa barton), are located in the Petit Lac (Figure 1). St Sulpice beaches (parc des pierrettes, plage de dorigny, plage de st-sulpice, saint-sulpice, tiger-duck beach), whose data is also used in this study, in the Grand Lac (Figure 1).\n",
    "\n",
    "#### Map here\n",
    "\n",
    "Lake Geneva is connected to the Mediterranean Sea by the Rhône River. The city of Port Saint Louis is at the mouth of the Rhône where it discharges into the sea. Port Saint Louis benefits from regular monitoring using the same protocol and coding system as Switzerland {cite}`merter`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data\n",
    "\n",
    "\n",
    "\n",
    "The first sample was recorded on Lake Geneva on November 23, 2015. Between November 2015 and June 2021 there were 247 beach-litter surveys at 38 different locations. In total, 78,104 objects were removed and identified, of which 358 objects were either tampons or tampon applicators (0.45%). {cite}`iqaasl` \n",
    "\n",
    "There are three separate sampling periods, each sampling period represents an initiative or campaign. The sampling periods are not of the same length nor is the sampling frequency fixed, except for a few specific locations in periods two and three. There were no samples collected in Geneva beaches during sampling period two. \n",
    "\n",
    "1. Project one: 2015-11-15 to 2017-03-01; the first project on Lac Léman (MCBP) \n",
    "\n",
    "2. Project two: 2017-04-01 to 2018-05-01; the Swiss Litter Report (SLR) \n",
    "\n",
    "3. Project three: 2020-03-01 to 2021-11-01; the start of IQAASL up to two weeks before the survey with End Period Plastics. \n",
    "\n",
    "FHP were present in 102 samples (41%). However, FHP are not found at the same rate at all locations. There were ten locations with only one sample (all periods included), of those ten at least one FHP was identified in four of the ten samples/locations. \n",
    "\n",
    "Of the other 28 locations, FHP were found at all locations except for three. Those three locations, in different regions of the lake, had two samples each. All locations with at least three surveys had at least one FHP identified in a survey, the minimum ratio of samples with FHP was 0.07 at baye-de-clarens. There were two locations that had a ratio of 1 (all samples have FHP products), la-pecherie in Allaman and parc-des-pierrettes in Saint Sulpice, see Annex table 1a. \n",
    "\n",
    "Table 1.  Number of locations, samples and positive samples per sampling period in Lake Geneva. Table 2. Number of locations, samples and positive samples per period for all locations within the city limits of Geneva city. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The shores of Saint Sulpice (Grand Lac) have been monitored yearly since 2016 by students of the EPFL.  \n",
    "\n",
    "The median beach litter survey value on Mediterranean Beaches was 294 p/100m {cite}`threshholdeu`. \n",
    "\n",
    "Statistical methods \n",
    "\n",
    "The location with the greatest probability, theta, of finding an FHP on a given sampling day will be determined by considering, for each location of interest, the data up to the day prior to sampling. The statistical conclusions about the parameter theta are made in terms of probability statements. {cite}`bayesgelman` \n",
    "\n",
    "Before any statement can be made about the relationship between the data and theta, they must be combined into a joint probability distribution. Bayes' rule does that by setting the conditional probability of theta to the observed data, p (θ|data) as: \n",
    "\n",
    "#### Formula one\n",
    "\n",
    "Note that the expression to the left of the equal sign reads \"the probability of theta given the data\", which is different from p( data | theta ). In p (θ|data),  theta is being conditioned by the observations, in p( data | theta ), the data is being conditioned by theta. {cite}`bayesdowney` \n",
    "\n",
    "These statements conform to a common-sense interpretation about any conclusions that may arise from the observed survey results.  To understand what is meant by common sense interpretation consider the series of questions that might be asked in order to gather the necessary information required to decide which location might have the greatest value of theta, prior to examining the data: \n",
    "\n",
    "* What is the value of theta for the lake? \n",
    "\n",
    "* Is it the same at all locations? \n",
    "\n",
    "* What is the value of theta Geneva? \n",
    "\n",
    "* Is it greater or less than the lake? \n",
    "\n",
    "* Is it the same at all locations in Geneva? \n",
    "\n",
    "* Where is the best chance of finding an FHP in Geneva if all the data for the lake is available? \n",
    "\n",
    "* Where is the best chance of finding an FHP in Geneva if only a subset of the data is available? \n",
    "\n",
    "* Do objects with similar sources have similar results? \n",
    "\n",
    "* Do other cities have similar results? \n",
    "\n",
    "* Were the samples gathered in the same way? \n",
    "\n",
    "* What is the difference between condition three and four? \n",
    "\n",
    "* Is four within the range of three? \n",
    "\n",
    "* Does the conclusion change given condition three or four? \n",
    "\n",
    "* Where am likely to find the most in the river basin?  How does Geneva compare? \n",
    "\n",
    "* Given the data, how reasonable are the answers to these questions? \n",
    "\n",
    "The uncertainty of those conclusions is defined by the range of the 94% Highest Density Interval of probability (HDI) as opposed to a confidence interval. The interval of probability contains the likely values of theta given the data and our prior beliefs. {cite}`bayesgelman` {cite}`bayesdowney` {cite}`bayespillon` \n",
    "\n",
    "The probability distribution of θ given the data, or posterior, can be defined in more general terms as: \n",
    "\n",
    "#### formula 2\n",
    "\n",
    "where the prior is the estimate or belief about theta prior to seeing the data and the likelihood is the chance of observing the data given theta, the normalizing constant is the total probability of the data and ensures that, when theta is integrated on [0,1], it integrates to 1. {cite}`bayesrules` \n",
    "\n",
    "To construct the model, the prior, posterior and likelihood functions need to be defined. The probability of finding an FHP anywhere on the lake ranges from [0,1] and is not constant. Thus, theta can take on different values depending on the quantity at the location of interest and the surveyor's ability to find and recognize the object as an FHP. \n",
    "\n",
    "The results for each survey are transformed to Boolean values. If an FHP was recorded at a survey, FHP = 1 and is 0 otherwise. This reduces each survey to one independent Bernoulli trial. Therefore, the probability of finding an FHP at any location in n samples can be described by the probability mass function of the Binomial distribution {cite}`bindistwiki`: \n",
    "\n",
    "#### formula 3\n",
    "\n",
    "In this configuration (3) theta only takes on one value. To account for the different possible values of theta, its' value can be set to the Beta distribution resulting in a Binomial distribution with parameter theta which is defined by a Beta distribution with parameters alpha and beta.\n",
    "\n",
    "#### formula 4\n",
    "\n",
    "#### formula 5\n",
    "\n",
    "The Beta distribution, defined on [0,1], takes two parameters, the number of times that an FHP was found = alpha, and the number of times that an FHP was not found = beta (n-FHP). A conjugate prior to the Binomial distribution, this model has many applications to any situation where the parameter of interest is in the range [0,1]. {cite}`bayesgelman` {cite}bayesjefferys {cite}`bayesrules` \n",
    "\n",
    "The probability density function of the Beta distribution is {cite}`betawiki`: \n",
    "\n",
    "#### formula 6\n",
    "\n",
    "When sampling started in 2015, there were no reference values for the region. Count surveys of litter data in the maritime environment had produced volumes of data but under significantly different conditions. Without reference values, we had no prior assumptions on the probability of finding an FHP and assumed that the probability was the same for all locations on the lake. The assumed distribution of theta prior to November 2015 is therefore a standard uniform distribution defined by Beta(1,1).  This reflects the experience and expectations at the time. {cite}`bayesjefferys` {cite}`catalogofpriors` {cite}`noninformativepriors` \n",
    "\n",
    "The general forms in equations 1 and 2 can now be defined using elements of 3 and 6, according to the model defined in 4 and 5. Note that the second and third expressions on the right side of the equations in 3 and 6 are the same for the Binomial and Beta distributions.\n",
    "\n",
    "#### formula 7\n",
    "\n",
    "#### formula 8 \n",
    "\n",
    "#### formula 9\n",
    "\n",
    "Putting that together by dropping the normalizing constants that do not depend on theta (10) and combining like values results in the posterior distribution of theta (11).\n",
    "\n",
    "#### formula 10\n",
    "\n",
    "#### formula 11\n",
    "\n",
    "The posterior distribution of theta (11) is an unnormalized Beta distribution with parameters alpha and beta.  This means that the prior uninformed estimate in November 2015 can be updated with the data from each sampling period in sequence. {cite}`bayesrules`  {cite}`bayesdowney` \n",
    "\n",
    "Using this method, the results from sampling period one (which incorporate the initial estimate) become the prior distribution for the results of sampling period two and this process is repeated until the last sampling period. {cite}`bayesdowney`\n",
    "\n",
    "__Assumptions__\n",
    "\n",
    "* The samples are independent and identically distributed\n",
    "* Theta is approximately equal for all locations which is the expected value for the lake\n",
    "* The expected result for the lake or the region is the best estimate for locations without samples\n",
    "* Exchangeability of data {cite}`bayesgelman` \n",
    "\n",
    "__Computational methods__\n",
    "\n",
    "Markov Chain Monte Carlo (MCMC) was used. MCMC is a general method used to simulate probability models. A Markov chain is a model that describes a sequence of possible events where the probability of each event depends only on the results of the previous event. {cite}`bayeskruschke`  {cite}`bayesdowney` {cite}`bayespilon` \n",
    "\n",
    "The implementation of MCMC methods is done with PyMC3 v3.1 {cite}`pymc` and the results are analyzed with ArviZ v0.11.4 {cite}`arviz`, SciPy v1.7 {cite}`scipy` and pandas v1.34 {cite}`reback2020pandas` all running in a Python v3.7 {cite}`python` environment. \n",
    "\n",
    "The data and methods are available in the repository: https://github.com/hammerdirt-analyst/finding-one-object \n",
    "\n",
    "## RESULTS AND DISCUSSION \n",
    "\n",
    "The probable mean values of theta lake (Figure 2, Table three) from one sampling period to the next is an example of how new data changes the expectation of finding an FHP on the beach as more samples are acquired. Consider that the difference between the 97% and 3% interval for each sampling period gets smaller as more data is added. Thus, decreasing uncertainty about the incidence of FHP on the lake and the probability of finding one, by year three it is 94% certain that the chance of finding an FHP on the beach is between 35% and 47% with an expected value of 41% (Table three).   \n",
    "\n",
    "As shown in Figure 2, it is apparent that the probability of finding FHPs in the whole lake was not the same at every survey. The expected probability is highest in year two, but the smallest 94% HDI is observed in year three: 12%. Recall that there were no samples in period two in Geneva and that not all locations were sampled in each sampling period (Table 2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plac = [\n",
    "    \"Prangins\",\n",
    "    \"Promenthoux\",\n",
    "    \"Nyon\",\n",
    "    \"Crans-près-Céligny\",\n",
    "    \"Céligny\", \n",
    "    \"Founex\",\n",
    "    \"Coppet\",\n",
    "    \"Tannay\",\n",
    "    \"Mies\",\n",
    "    \"Versoix\",\n",
    "    \"Genthod\",\n",
    "    \"Bellevue\",\n",
    "    \"Pregny-Chambésy\",\n",
    "    \"Hermance\",\n",
    "    \"Anières\",\n",
    "    \"Corsier\",\n",
    "    \"Collonge-Bellerive\",\n",
    "    \"Vésenaz\",\n",
    "    \"Cologny\",\n",
    "    \"Genève\",]\n",
    "plac_beaches = dfBeaches[dfBeaches.city.isin(plac)].index.to_numpy()\n",
    "\n",
    "glac = [\n",
    "    \"Bourg-en-Lavaux\",\n",
    "    \"Puidoux\", \n",
    "    \"Cully\", \n",
    "    \"Grandvaux\", \n",
    "    \"Villette (Lavaux)\", \n",
    "    \"Lutry\", \n",
    "    \"Paudex\", \n",
    "    \"Pully\", \n",
    "    \"Lausanne\", \n",
    "    \"St-Sulpice (VD)\", \n",
    "    \"Préverenges\", \n",
    "    \"Morges\", \n",
    "    \"Tolochenaz\", \n",
    "    \"St-Prex\", \n",
    "    \"Buchillon\", \n",
    "    \"Allaman\", \n",
    "    \"Perroy\", \n",
    "    \"Rolle\", \n",
    "    \"Bursinel\", \n",
    "    \"Dully\", \n",
    "    \"Gland\",\n",
    "]\n",
    "glac_beaches = dfBeaches[dfBeaches.city.isin(glac)].index.to_numpy()\n",
    "\n",
    "hlac = [\n",
    "    \"Noville\",\n",
    "    \"Villeneuve (VD)\",\n",
    "    \"Veytaux\",\n",
    "    \"Territet\",\n",
    "    \"Montreux\",\n",
    "    \"Clarens\",\n",
    "    \"La Tour-de-Peilz\",\n",
    "    \"Vevey\",\n",
    "    \"Corseaux\",\n",
    "    \"St-Saphorin (Lavaux)\",\n",
    "    \"Rivaz\",\n",
    "    \"Port-Valais\",\n",
    "    \"Saint-Gingolph\"    \n",
    "]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# fhg_summary[\"rate\"] = fhg_summary.s_pos/fhg_summary[\"n samples\"]\n",
    "\n",
    "# assign a period to each sample\n",
    "ldb['period'] = 1\n",
    "ldb.loc[y_two, 'period'] = 2\n",
    "ldb.loc[y_thre, 'period'] = 3\n",
    "\n",
    "# identify the zero values\n",
    "just_fhg = ldb[fhg].copy()\n",
    "just_fhg['found'] = False\n",
    "just_fhg.loc[(just_fhg.quantity > 0), 'found'] = True\n",
    "\n",
    "# count the positive values y for each period and region\n",
    "geneva = just_fhg[just_fhg.city.isin(city)]\n",
    "sulpice = just_fhg[just_fhg.city.isin(['Saint-Sulpice (VD)'])]\n",
    "p_lac = just_fhg[just_fhg.city.isin(plac)]\n",
    "\n",
    "summary_sulpice = sulpice.groupby('period', as_index=False).agg({'location':'nunique', 'loc_date':'nunique', 'found':'sum'})\n",
    "summary_sulpice.rename(columns={'loc_date':'samples','found':'FHP', 'location':'locations'}, inplace=True)\n",
    "summary_sulpice.sort_values(by='period', inplace=True)\n",
    "summary_sulpice.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "summary_geneva = geneva.groupby('period', as_index=False).agg({'location':'nunique', 'loc_date':'nunique', 'found':'sum'})\n",
    "summary_geneva.rename(columns={'loc_date':'samples','found':'FHP', 'location':'locations'}, inplace=True)\n",
    "\n",
    "#there were  no samples in Geneva durring the second sampling period\n",
    "summary_geneva.loc[2] = 2,0, 0, 0\n",
    "summary_geneva.sort_values(by='period', inplace=True)\n",
    "summary_geneva.reset_index(drop=True, inplace=True)\n",
    "\n",
    "summary_just_fhg = just_fhg.groupby('period', as_index=False).agg({'location':'nunique', 'loc_date':'nunique', 'found':'sum'})\n",
    "summary_just_fhg.rename(columns={'loc_date':'samples','found':'FHP', 'location':'locations'}, inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots(2,2, figsize=(9,9))\n",
    "\n",
    "axone = ax[0,0]\n",
    "axtwo= ax[0,1]\n",
    "axthree=ax[1,0]\n",
    "\n",
    "table_one = a_simple_formatted_table(axone,summary_just_fhg.values,colLabels=summary_just_fhg.columns, a_color=\"black\", colWidths=[*[.25]*4], bbox=[0,0,1,1])\n",
    "axone = remove_spines(axone)\n",
    "axone = remove_ticks(axone)\n",
    "# axone.set_xlabel(\"Lac Léman\", fontsize=14, labelpad=14)\n",
    "axone.set_title(\"Table 1: Lac Léman\", fontsize=14, loc=\"left\")\n",
    "\n",
    "table_two = a_simple_formatted_table(axtwo,summary_geneva.values,colLabels=summary_geneva.columns, a_color=\"black\", colWidths=[*[.25]*4], bbox=[0,0,1,1],)\n",
    "axtwo = remove_spines(axtwo)\n",
    "axtwo = remove_ticks(axtwo)\n",
    "#axtwo.set_xlabel(\"Geneva\", fontsize=14, labelpad=14)\n",
    "axtwo.set_title(\"Table 2: Geneva\", fontsize=14, loc=\"left\")\n",
    "\n",
    "table_three = a_simple_formatted_table(axthree,summary_sulpice.values,colLabels=summary_sulpice.columns, a_color=\"black\", colWidths=[*[.25]*4], bbox=[0,0,1,1],)\n",
    "axthree = remove_spines(axthree)\n",
    "axthree = remove_ticks(axthree)\n",
    "# axthree.set_xlabel(\"St Sulpice\", fontsize=14, labelpad=14)\n",
    "axthree.set_title(\"Table 3: Saint Sulpice\", fontsize=14, loc=\"left\")\n",
    "ax[1,1].axis(\"off\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period</th>\n",
       "      <th>loc_date</th>\n",
       "      <th>found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   period  loc_date  found\n",
       "0       1         9      0\n",
       "1       3         2      1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_lac[p_lac.location == \"villa-barton\"].groupby('period', as_index=False).agg({'loc_date':'nunique', 'found':'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period</th>\n",
       "      <th>location</th>\n",
       "      <th>loc_date</th>\n",
       "      <th>found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   period  location  loc_date  found\n",
       "0       1         3        13      3\n",
       "1       3         5        19      4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_lac.groupby('period', as_index=False).agg({'location':'nunique', 'loc_date':'nunique', 'found':'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['villa-barton', 'baby-plage-geneva', 'versoix', 'jardin-botanique',\n",
       "       'baby-plage-ii-geneve', 'rocky-plage'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_lac.location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period</th>\n",
       "      <th>loc_date</th>\n",
       "      <th>found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   period  loc_date  found\n",
       "0       3        11      3"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_lac[p_lac.location == 'baby-plage-geneva'].groupby('period', as_index=False).agg({'loc_date':'nunique', 'found':'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2727272727272727"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3/11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of an fhp on the lake is\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import binom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229 249\n"
     ]
    }
   ],
   "source": [
    "g27 = ldb[ldb.code.isin([\"G27\"])].copy()\n",
    "print(g27.fail.sum(), g27.loc_date.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(0.21428571), array(0.00580577))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = beta(6, 22)\n",
    "beta.stats(6, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "15/40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9196787148594378"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>code</th>\n",
       "      <th>pcs_m</th>\n",
       "      <th>quantity</th>\n",
       "      <th>location</th>\n",
       "      <th>loc_date</th>\n",
       "      <th>water_name_slug</th>\n",
       "      <th>river_bassin</th>\n",
       "      <th>length</th>\n",
       "      <th>...</th>\n",
       "      <th>% to water</th>\n",
       "      <th>% to unproductive</th>\n",
       "      <th>streets</th>\n",
       "      <th>month</th>\n",
       "      <th>eom</th>\n",
       "      <th>material</th>\n",
       "      <th>w_t</th>\n",
       "      <th>streets km</th>\n",
       "      <th>p/100m</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>140</td>\n",
       "      <td>2016-07-22</td>\n",
       "      <td>G10</td>\n",
       "      <td>0.43</td>\n",
       "      <td>6</td>\n",
       "      <td>villa-barton</td>\n",
       "      <td>(villa-barton, 2016-07-22)</td>\n",
       "      <td>lac-leman</td>\n",
       "      <td>rhone</td>\n",
       "      <td>14.019905</td>\n",
       "      <td>...</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85080</td>\n",
       "      <td>7</td>\n",
       "      <td>2016-07-31</td>\n",
       "      <td>Plastic</td>\n",
       "      <td>l</td>\n",
       "      <td>85.080</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>141</td>\n",
       "      <td>2016-07-22</td>\n",
       "      <td>G34</td>\n",
       "      <td>0.29</td>\n",
       "      <td>4</td>\n",
       "      <td>villa-barton</td>\n",
       "      <td>(villa-barton, 2016-07-22)</td>\n",
       "      <td>lac-leman</td>\n",
       "      <td>rhone</td>\n",
       "      <td>14.019905</td>\n",
       "      <td>...</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85080</td>\n",
       "      <td>7</td>\n",
       "      <td>2016-07-31</td>\n",
       "      <td>Plastic</td>\n",
       "      <td>l</td>\n",
       "      <td>85.080</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>142</td>\n",
       "      <td>2016-07-22</td>\n",
       "      <td>G175</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2</td>\n",
       "      <td>villa-barton</td>\n",
       "      <td>(villa-barton, 2016-07-22)</td>\n",
       "      <td>lac-leman</td>\n",
       "      <td>rhone</td>\n",
       "      <td>14.019905</td>\n",
       "      <td>...</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85080</td>\n",
       "      <td>7</td>\n",
       "      <td>2016-07-31</td>\n",
       "      <td>Metal</td>\n",
       "      <td>l</td>\n",
       "      <td>85.080</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>143</td>\n",
       "      <td>2016-07-22</td>\n",
       "      <td>G66</td>\n",
       "      <td>0.36</td>\n",
       "      <td>5</td>\n",
       "      <td>villa-barton</td>\n",
       "      <td>(villa-barton, 2016-07-22)</td>\n",
       "      <td>lac-leman</td>\n",
       "      <td>rhone</td>\n",
       "      <td>14.019905</td>\n",
       "      <td>...</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85080</td>\n",
       "      <td>7</td>\n",
       "      <td>2016-07-31</td>\n",
       "      <td>Plastic</td>\n",
       "      <td>l</td>\n",
       "      <td>85.080</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>144</td>\n",
       "      <td>2016-07-22</td>\n",
       "      <td>G182</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2</td>\n",
       "      <td>villa-barton</td>\n",
       "      <td>(villa-barton, 2016-07-22)</td>\n",
       "      <td>lac-leman</td>\n",
       "      <td>rhone</td>\n",
       "      <td>14.019905</td>\n",
       "      <td>...</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85080</td>\n",
       "      <td>7</td>\n",
       "      <td>2016-07-31</td>\n",
       "      <td>Metal</td>\n",
       "      <td>l</td>\n",
       "      <td>85.080</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260414</th>\n",
       "      <td>339426</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>G166</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>la-morges</td>\n",
       "      <td>(la-morges, 2017-04-09)</td>\n",
       "      <td>lac-leman</td>\n",
       "      <td>rhone</td>\n",
       "      <td>30.195382</td>\n",
       "      <td>...</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72218</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-04-30</td>\n",
       "      <td>Wood</td>\n",
       "      <td>l</td>\n",
       "      <td>72.218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260415</th>\n",
       "      <td>339427</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>G180</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>la-morges</td>\n",
       "      <td>(la-morges, 2017-04-09)</td>\n",
       "      <td>lac-leman</td>\n",
       "      <td>rhone</td>\n",
       "      <td>30.195382</td>\n",
       "      <td>...</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72218</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-04-30</td>\n",
       "      <td>Metal</td>\n",
       "      <td>l</td>\n",
       "      <td>72.218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260416</th>\n",
       "      <td>339428</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>G56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>la-morges</td>\n",
       "      <td>(la-morges, 2017-04-09)</td>\n",
       "      <td>lac-leman</td>\n",
       "      <td>rhone</td>\n",
       "      <td>30.195382</td>\n",
       "      <td>...</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72218</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-04-30</td>\n",
       "      <td>Plastic</td>\n",
       "      <td>l</td>\n",
       "      <td>72.218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260417</th>\n",
       "      <td>339429</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>G88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>la-morges</td>\n",
       "      <td>(la-morges, 2017-04-09)</td>\n",
       "      <td>lac-leman</td>\n",
       "      <td>rhone</td>\n",
       "      <td>30.195382</td>\n",
       "      <td>...</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72218</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-04-30</td>\n",
       "      <td>Plastic</td>\n",
       "      <td>l</td>\n",
       "      <td>72.218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260418</th>\n",
       "      <td>339430</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>G205</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>la-morges</td>\n",
       "      <td>(la-morges, 2017-04-09)</td>\n",
       "      <td>lac-leman</td>\n",
       "      <td>rhone</td>\n",
       "      <td>30.195382</td>\n",
       "      <td>...</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72218</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-04-30</td>\n",
       "      <td>Glass</td>\n",
       "      <td>l</td>\n",
       "      <td>72.218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33600 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0       date  code  pcs_m  quantity      location  \\\n",
       "140            140 2016-07-22   G10   0.43         6  villa-barton   \n",
       "141            141 2016-07-22   G34   0.29         4  villa-barton   \n",
       "142            142 2016-07-22  G175   0.14         2  villa-barton   \n",
       "143            143 2016-07-22   G66   0.36         5  villa-barton   \n",
       "144            144 2016-07-22  G182   0.14         2  villa-barton   \n",
       "...            ...        ...   ...    ...       ...           ...   \n",
       "260414      339426 2017-04-09  G166   0.00         0     la-morges   \n",
       "260415      339427 2017-04-09  G180   0.00         0     la-morges   \n",
       "260416      339428 2017-04-09   G56   0.00         0     la-morges   \n",
       "260417      339429 2017-04-09   G88   0.00         0     la-morges   \n",
       "260418      339430 2017-04-09  G205   0.00         0     la-morges   \n",
       "\n",
       "                          loc_date water_name_slug river_bassin     length  \\\n",
       "140     (villa-barton, 2016-07-22)       lac-leman        rhone  14.019905   \n",
       "141     (villa-barton, 2016-07-22)       lac-leman        rhone  14.019905   \n",
       "142     (villa-barton, 2016-07-22)       lac-leman        rhone  14.019905   \n",
       "143     (villa-barton, 2016-07-22)       lac-leman        rhone  14.019905   \n",
       "144     (villa-barton, 2016-07-22)       lac-leman        rhone  14.019905   \n",
       "...                            ...             ...          ...        ...   \n",
       "260414     (la-morges, 2017-04-09)       lac-leman        rhone  30.195382   \n",
       "260415     (la-morges, 2017-04-09)       lac-leman        rhone  30.195382   \n",
       "260416     (la-morges, 2017-04-09)       lac-leman        rhone  30.195382   \n",
       "260417     (la-morges, 2017-04-09)       lac-leman        rhone  30.195382   \n",
       "260418     (la-morges, 2017-04-09)       lac-leman        rhone  30.195382   \n",
       "\n",
       "        ...  % to water % to unproductive streets  month         eom  \\\n",
       "140     ...    1.133333               0.0   85080      7  2016-07-31   \n",
       "141     ...    1.133333               0.0   85080      7  2016-07-31   \n",
       "142     ...    1.133333               0.0   85080      7  2016-07-31   \n",
       "143     ...    1.133333               0.0   85080      7  2016-07-31   \n",
       "144     ...    1.133333               0.0   85080      7  2016-07-31   \n",
       "...     ...         ...               ...     ...    ...         ...   \n",
       "260414  ...    1.166667               0.0   72218      4  2017-04-30   \n",
       "260415  ...    1.166667               0.0   72218      4  2017-04-30   \n",
       "260416  ...    1.166667               0.0   72218      4  2017-04-30   \n",
       "260417  ...    1.166667               0.0   72218      4  2017-04-30   \n",
       "260418  ...    1.166667               0.0   72218      4  2017-04-30   \n",
       "\n",
       "        material  w_t  streets km  p/100m  year  \n",
       "140      Plastic    l      85.080    43.0  2016  \n",
       "141      Plastic    l      85.080    29.0  2016  \n",
       "142        Metal    l      85.080    14.0  2016  \n",
       "143      Plastic    l      85.080    36.0  2016  \n",
       "144        Metal    l      85.080    14.0  2016  \n",
       "...          ...  ...         ...     ...   ...  \n",
       "260414      Wood    l      72.218     0.0  2017  \n",
       "260415     Metal    l      72.218     0.0  2017  \n",
       "260416   Plastic    l      72.218     0.0  2017  \n",
       "260417   Plastic    l      72.218     0.0  2017  \n",
       "260418     Glass    l      72.218     0.0  2017  \n",
       "\n",
       "[33600 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xlake = pd.concat(dfs[:-1])\n",
    "Xlake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of an fhp on the lake is\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import binom\n",
    "\n",
    "# testing Binary Hypotheses with binary data\n",
    "# p(-H|DX) = p(-H|X) * p(D|-HX)/p(D|X)\n",
    "# H = the proposition or hypothesis\n",
    "# H,-H = True, False\n",
    "# D = data, information => the survey results from the last sampling period\n",
    "# up to the day prior to the excercise\n",
    "# X = prior data\n",
    "\n",
    "prior_periods = ldb.period.isin([1,20])\n",
    "\n",
    "Xlake = ldb[prior_periods].copy()\n",
    "Xgeneva = ldb[(prior_periods)&(ldb.city.isin(city))].copy()\n",
    "Xsaint = ldb[(prior_periods)&(ldb.city.isin([\"St-Sulpice (VD)\"]))].copy()\n",
    "\n",
    "Dlake = ldb\n",
    "\n",
    "\n",
    "\n",
    "# add up year one and two\n",
    "samples = summary_just_fhg.loc[:, \"samples\"].sum()\n",
    "fhp= summary_just_fhg.loc[:, \"FHP\"].sum()\n",
    "noFHP=samples-fhp\n",
    "\n",
    "prior = beta(noFHP, fhp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Method\n",
    "\n",
    "The location with the greatest probability of finding an FHP will be determined by Bayesian inference. The statistical conclusions about a parameter $\\theta$ are made in terms of probability statements. These statements conform to a commom-senses interpretation about any conclusions that may arise from the statistical analysis.  Using the Bayesian method the results of the analysis will provide an __interval of probability__ as opposed to a confidence interval. The interval of probability contains the likely values of $\\theta$ given the data and our __prior beliefs__. {cite}`gelman` {cite}`pillon` {cite}`downey`\n",
    "\n",
    "Before any probability statements can be made about the relationship between the data and $\\theta$, they must be combined into a joint probability distribution. Bayes' rule does that by setting the conditional probablilty of $\\theta$ to the observed data:\n",
    "\n",
    "$$p({\\theta} | data) = \\frac{p(\\theta)p(data | \\theta)}{p(data)} \\tag{1}$$\n",
    "\n",
    "The expression to the left of the equal sign is read _\"The probability of $\\theta$ given data\"_, which is the unobserved parameter of interest. The numerator is the product of the probability of $\\theta$ and the probability of the data given $\\theta$. {cite}`gelman`\n",
    "\n",
    "The equality can be expressed in more general terms:\n",
    "\n",
    "$$\\text{posterior} = \\frac{\\text{prior * likelihood}}{\\text{normalizing constant}} \\tag{2}$$\n",
    "\n",
    "The terms prior, posterior and likelihood have specific meanings. The prior is the estimate or belief about $\\theta$ prior to collecting data. The likelihood is the chance of observing the data given $\\theta$. The posterior is the distribution of the unobserved parameter ($\\theta$), the value of interest. The normalizing constant is the total probability of the data and ensures that when $\\theta$ is integrated on \\[0,1\\] it actually integrates to 1.\n",
    "\n",
    "__The model__\n",
    "\n",
    "To construct the model the prior, posterior and likelihood functions need to be defined. The probability of finding an FHP anywhere on the lake ranges from \\[0,1\\] and is not constant. Thus $\\theta$ can take on different values depending on the quantity at the location of interest and the surveyors ability to find and recognize the object as an FHP.\n",
    "\n",
    "Before defining the model the results for each survey need to be transformed to boolean values. If an FHP was recorded at a survey FHP = 1 and 0 otherwise. This reduces each survey to one idependent bernouli trial. Therefore, the probability of finding an FHP at any location can be described by the Binomial distribution, where n=number of samples and FHP=the number of times an FHP was found:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(FHP|n,\\theta) &= {n \\choose FHP} \\theta^{FHP} (1-\\theta)^{ n-FHP}\\\\\n",
    "\\tag{3}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The above expression states that \"_The probability of FHP found given the number of samples and $\\theta$ equals the probability density function of the Binomial distribution_\". In this configuration $\\theta$ only takes on one value. To account for the possible different values, $\\theta$ can be set to a probability distribution that is defined on the range \\[0,1\\]. The Beta-Binomial model defines the probability of an event (finding an FHP) on \\[0, 1\\] as the function of two parameters $\\alpha$ and $\\beta$. By defining the value of $\\theta$ as a Beta distribution with parameters the variability between locations can be better identified :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "FHP|\\theta \\sim & Bin(n, \\theta)\\\\[8pt]\n",
    "\\theta \\sim & Beta(\\alpha, \\beta)\n",
    "\\tag{4}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The Beta distribtion is a *conjugate prior* to the Binomial distribution and is limited to values between 0 and 1. This results in a closed form __posterior distribution__ and predictable methods for updating the parameters of Beta from one sampling period to the next. This model has many applications to any situtation where the parameter of interest is in the range \\[0,1\\]. {cite}`gelman` {cite}`jefferies` {cite}`bayesrules`\n",
    "\n",
    "The probability density function of the Beta distribution is:\n",
    "\n",
    "$$f(\\theta | \\alpha, \\beta) = \\frac{1}{Beta(\\alpha, \\beta)} \\theta^{\\alpha -1} (1-\\theta)^{\\beta - 1} \\tag{5}$$\n",
    "\n",
    "The general forms in 1 and 2 can now be defined using elements of 3, 4 and 5. The normalizing constant is not dependent on $\\theta$ in either the Beta or Binomial distributions. \n",
    "\n",
    "__The prior__\n",
    "\n",
    "When sampling started in 2015 there were no reference values for the region. Count surveys of litter data in the maritime environment had produced volumes of data but under significantly different conditions. Without reference values we had no prior assumptions on the probability of finding an FHP and assumed the probability was $\\approxeq$ for all locations on the lake.\n",
    "\n",
    "The assumed distribution of $\\theta$ **prior** to November 2015 is therefore $Beta(1,1) = \\frac{1}{Beta(1, 1)} \\theta^{\\alpha -1} (1-\\theta)^{\\beta - 1} = 1$ which is the standard uniform distribution. Thus reflecting the experience and expectations when sampling started.\n",
    "\n",
    "__The probability of finding an FHP__ where n=number of samples, FHP=the number of samples with at least one FHP and $\\theta$ is the probability of finding at least one FHP at a sample:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(\\theta | data) &=  \\frac{1}{Beta(\\alpha, \\beta)} \\theta^{\\alpha -1} (1-\\theta)^{\\beta - 1} \\cdot {n \\choose FHP} \\theta^{FHP} (1-\\theta)^{ n-FHP} \\tag{6}\\\\[8pt]\n",
    "&\\propto \\theta^{\\alpha -1} (1-\\theta)^{\\beta - 1} \\cdot \\theta^{FHP} (1-\\theta)^{ n-FHP} \\tag{7}\\\\[8pt]\n",
    "&\\propto \\theta^{(\\alpha + FHP) - 1} (1-\\theta)^{(\\beta + n - FHP) - 1} \\tag{8}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The normalizing constants that do not depend on $\\theta$ are dropped (7) the result is the unormalized posterior distribution (8). It is important to note that (5) and (8) share the same form, the normalized Beta distribution. \n",
    "\n",
    "__Updating the model__\n",
    "\n",
    "The posterior distribution is an unormalized Beta distribution with parameters $\\alpha + FHP$ and $\\beta + n - FHP$. This means that the **prior** uninformed estimate in November 2015 can be updated with the data from each sampling period in sequence. Using this method the results from sampling period one (which incorporate the initial estimate) become the **prior** distribution for the results of sampling period two and this process is repeated until the last sampling period. For example:\n",
    "\n",
    "_Sampling period one example_\n",
    "\n",
    "$\\theta \\sim Beta(1,1)$ = estimate $\\theta$ prior to sampling\n",
    "\n",
    "n - y = number of samples in period one WITHOUT an FHP\n",
    "\n",
    "y = number of samples in period one WITH at leat one FHP\n",
    "\n",
    "_apply n and y from the observed results to the initial estimate:_\n",
    "\n",
    "$\\theta \\sim Beta( \\beta = 1 + y, \\alpha = 1 + n - y)$ = the estimated probability after period one **and** the prior distribution (initial estimate) for period two.\n",
    "\n",
    "__Assumptions__\n",
    "\n",
    "1. The samples are independent and identically distributed\n",
    "2. $\\theta$ $\\approxeq$ for all locations which is the expected value for the lake\n",
    "3. The expected result for the lake or the region is the best estimate for locations without samples\n",
    "4. exchangeabilty of data\n",
    "\n",
    "__Computational methods__\n",
    "\n",
    "Markov Chain Monte Carlo (MCMC) is a general method used to simulate probability models.  A Markov chain is a model that describes a sequence of possible events where the probability of each event depends only on the results of the previous event. {cite}`stan` {cite}`bayesrules` {cite}`pillon`\n",
    "\n",
    "The implementation of MCMC methods is done with PyMC3 v3.1 {cite}`pymc` and the results are analysed with ArviZ  v0.11.4 {cite}`arviz`, SciPy v1.7 {cite}`scipy`\n",
    "and pandas v1.34 {cite}`pandas` all running in a Python v3.7 {cite}`python` environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "__Lake Geneva__\n",
    "\n",
    "Before estimating the probable values of $\\theta$ for a specific location the third assumption needs to be calculated for each sampling period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_20390/668683560.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Parameters\n",
    "\n",
    "    psi: float\n",
    "\n",
    "        Expected proportion of Binomial variates (0 < psi < 1)\n",
    "    n: int\n",
    "\n",
    "        Number of Bernoulli trials (n >= 0).\n",
    "    p: float\n",
    "\n",
    "        Probability of success in each trial (0 < p < 1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "def model_a_sampling_period(observed, FHP, pk, names=[\"pLake\", \"thetaLake\"], model_name=[\"yeartwo\"], draws=5000):\n",
    "    \n",
    "    with pm.Model() as a:\n",
    "    \n",
    "        plake = pm.Beta(names[0], alpha=FHP, beta=pk)        \n",
    "        thetaLake = pm.Binomial(names[1],  n=1, observed=observed, p=plake)\n",
    "        \n",
    "        if 0 < np.mean(observed) < 1:\n",
    "            start_here = np.mean(observed)\n",
    "        else:\n",
    "            start_here = 0.4\n",
    "        \n",
    "\n",
    "        trace = pm.sample(draws=draws, cores=4, tune=1000, return_inferencedata=False, progressbar = False, start={names[1]:[start_here]})\n",
    "        postpred = pm.sample_posterior_predictive(trace, progressbar = False, var_names=names)\n",
    "        prior = pm.sample_prior_predictive()    \n",
    "\n",
    "        priordata = az.from_pymc3(\n",
    "            trace = trace,\n",
    "            posterior_predictive = postpred,\n",
    "            prior = prior\n",
    "\n",
    "        )\n",
    "\n",
    "       \n",
    "    return {model_name[0]:priordata, \"model\": a}\n",
    "\n",
    "def parameterize_prior(model, var_name=\"pLake\"):\n",
    "    \"\"\"Makes alpha, beta parameters from the mean and standard deviation\n",
    "    of the prior distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    e = model[\"posterior_predictive\"][var_name].values.flatten()\n",
    "    f = len(e)\n",
    "    estd = np.std(e)\n",
    "    k=(((np.mean(e))*(1-np.mean(e)))/(estd**2))-1\n",
    "    \n",
    "    a=(np.mean(e)*k)\n",
    "    b=(1-np.mean(e))*k\n",
    "    \n",
    "    return a, b\n",
    "\n",
    "# model variables\n",
    "# observed\n",
    "observed = year_one[year_one.code.isin(code)].groupby(\"loc_date\").fail.sum().values\n",
    "\n",
    "# sometimes both types of FHP are found\n",
    "# make everything 0 or 1\n",
    "observed[observed > 1] = 1\n",
    "\n",
    "# the first year of sampling we didn't\n",
    "# know what to epxpect\n",
    "FHP = 1\n",
    "pk = 1\n",
    "\n",
    "year_one_model = model_a_sampling_period(observed, FHP, pk, names=[\"pLake\", \"thetaLake\"], model_name=[\"yearone\"])\n",
    "\n",
    "# year two\n",
    "observed = year_two[year_two.code.isin(code)].groupby(\"loc_date\").fail.sum().values\n",
    "\n",
    "observed[observed > 1] = 1\n",
    "\n",
    "FHP, pk = parameterize_prior(year_one_model[\"yearone\"], var_name=\"pLake\")\n",
    "print(FHP, pk)\n",
    "\n",
    "year_two_model = model_a_sampling_period(observed, FHP, pk, names=[\"pLake\", \"thetaLake\"], model_name=[\"yeartwo\"])\n",
    "\n",
    "# year three\n",
    "observed = year_three[year_three.code.isin(code)].groupby(\"loc_date\").fail.sum().values\n",
    "\n",
    "observed[observed > 1] = 1\n",
    "\n",
    "# year two is the prior for year three\n",
    "FHP, pk = parameterize_prior(year_two_model[\"yeartwo\"], var_name=\"pLake\")\n",
    "\n",
    "year_three_model = model_a_sampling_period(observed, FHP, pk, names=[\"pLake\", \"thetaLake\"], model_name=[\"yearthree\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FHP, pk = parameterize_prior(year_one_model[\"yearone\"], var_name=\"pLake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameterize_prior(year_two_model[\"yeartwo\"], var_name=\"pLake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_two_model[\"yeartwo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "posts = {}\n",
    "models = dict(yearone=year_one_model, yeartwo=year_two_model, yearthree=year_three_model)\n",
    "\n",
    "# plot data\n",
    "for a_model in models:\n",
    "    data = models[a_model][a_model][\"posterior_predictive\"][\"pLake\"].values\n",
    "    sd=[*data[0], *data[1], *data[2], *data[3]]\n",
    "    \n",
    "    posts.update({a_model:sd})\n",
    "\n",
    "# summary stats\n",
    "tables = []\n",
    "for a_model in models:\n",
    "    data = az.summary(models[a_model][a_model], fmt=\"long\")\n",
    "    data.rename(columns={\"pLake\":a_model}, inplace=True)\n",
    "    tables.append(data)\n",
    "\n",
    "table_data = pd.concat(tables, axis=1)\n",
    "table_data.reset_index(inplace=True)\n",
    "\n",
    "fig, axs= plt.subplots(1,2, figsize=(10,6))\n",
    "\n",
    "# ythr_pos = lake_posteriors[2]\n",
    "\n",
    "wght = len(posts[\"yearone\"])\n",
    "\n",
    "axone=axs[0]\n",
    "axtwo=axs[1]\n",
    "\n",
    "sns.histplot(np.sort(posts[\"yearone\"]), stat='probability', color= 'blue', bins=50,binwidth=.021, alpha=.4, ax=axone, zorder=0, label='Period one')\n",
    "sns.histplot(np.sort(posts[\"yeartwo\"]), stat='probability', color='black',  bins=50, binwidth=.021, alpha=0.4, ax=axone, zorder=0,label='Period two')\n",
    "sns.histplot(np.sort(posts[\"yearthree\"]), stat='probability', ax=axone, bins=50, binwidth=.018, zorder=2, alpha=1, label=\"period three\")\n",
    "\n",
    "# axone.vlines(x=ythr_pos.interval(.95)[0], ymin=0, ymax=0.06, color='red')\n",
    "# axone.vlines(x=ythr_pos.interval(.95)[1], ymin=0, ymax=0.06, color='red')\n",
    "# axone.axvspan(xmin=ythr_pos.interval(.95)[0], xmax=ythr_pos.interval(.95)[1], ymin=0, ymax=.5, fill='x', zorder=100, alpha=0.2, color='red', label=\"95% ETI\")\n",
    "\n",
    "axone.set_xlim(.01,.99)\n",
    "axone.set_xlabel(r\"$\\theta$\", fontsize=16)\n",
    "axone.set_ylabel(r\"Probability of $\\theta$\", fontsize=16)\n",
    "\n",
    "axone.legend()\n",
    "\n",
    "table_two = a_simple_formatted_table(axtwo,table_data.values,colLabels=table_data.columns, a_color=\"black\", colWidths=[*[.25]*4], bbox=[0,0,1,1])\n",
    "axtwo = remove_spines(axtwo)\n",
    "axtwo = remove_ticks(axtwo)\n",
    "# axtwo.set_xlabel(\"Geneva\", fontsize=14, labelpad=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Above:__ Figure 1, the distribution of $\\theta_{FHP}$ on Lake Geneva for each sampling period. At the end of year one it was apparent that the probability of finding FHPs was not the same at every survey. The expected probability is highest in year two, but the 94% HDI has the smallest range in year three ( 11.7 ).* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Geneva__\n",
    "\n",
    "Recall that there were no samples in period two and that not all locations are sampled in each sampling period (table two). The assumpion is that without other evidence, $\\theta$ at any location is $\\approxeq$ to $\\theta$ for the lake. This concept is reciprocal, if $\\theta$ lake is under consideration all locations that have valid samples on the lake are included in the estimation, (figure 1). Evidence is defined as sample results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Parameters\n",
    "\n",
    "    psi: float\n",
    "\n",
    "        Expected proportion of Binomial variates (0 < psi < 1)\n",
    "    n: int\n",
    "\n",
    "        Number of Bernoulli trials (n >= 0).\n",
    "    p: float\n",
    "\n",
    "        Probability of success in each trial (0 < p < 1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the aggregated survey data\n",
    "ldng = ldb[(ldb.code.isin(code))][[\"period\",\"location\", \"fail\", \"loc_date\"]].copy()\n",
    "ldng = ldng.groupby([\"loc_date\"], as_index=False).fail.sum()\n",
    "g = ldng[\"fail\"].values\n",
    "g[g > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldng.fail.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_a_sampling_period_x(observed, FHP, pk, psi=0.01, names=[\"pLake\", \"thetaLake\"], model_name=[\"a_model\"], draws=5000):\n",
    "    \n",
    "    with pm.Model() as a:\n",
    "        \n",
    "       \n",
    "    \n",
    "        plake = pm.Beta(names[0], alpha=FHP, beta=pk)        \n",
    "        # thetaLake = pm.Binomial(names[1],  n=1, observed=observed, p=plake)\n",
    "        zinfBlake = pm.ZeroInflatedBinomial(names[1], psi=psi, n=len(observed), p=plake)\n",
    "        \n",
    "        if 0 < np.mean(observed) < 1:\n",
    "            start_here = np.mean(observed)\n",
    "        else:\n",
    "            start_here = 0.4\n",
    "        \n",
    "\n",
    "        trace = pm.sample(draws=draws, cores=4, tune=1000, return_inferencedata=False, progressbar = False)\n",
    "        postpred = pm.sample_posterior_predictive(trace, progressbar = False, var_names=names)\n",
    "        prior = pm.sample_prior_predictive()    \n",
    "\n",
    "        priordata = az.from_pymc3(\n",
    "            trace = trace,\n",
    "            posterior_predictive = postpred,\n",
    "            prior = prior\n",
    "\n",
    "        )\n",
    "\n",
    "       \n",
    "    return {model_name[0]:priordata, \"model\": a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = ldng.fail\n",
    "fhp = ldng.fail.sum()\n",
    "pk = len(obs) - fhp\n",
    "\n",
    "print(fhp, pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = model_a_sampling_period_x(obs, fhp, pk, psi=0.4, names=[\"pLake\", \"thetaLake\"], model_name=[\"a_model\"], draws=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hm[\"model\"]:\n",
    "    az.plot_trace(hm[\"a_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_sampling_period_x(observed, FHP, pk, psi=0.01, names=[\"pLake\", \"thetaLake\", f\"z-{thetaunicode}\"], model_name=[\"a_model\"], draws=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymc3.distributions.discrete.ZeroInflatedBinomial(name, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# the locations of interest\n",
    "locs = dfBeaches[(dfBeaches.city == \"Genève\")&(dfBeaches.water == \"l\")].index.unique()\n",
    "thetaunicode = \"\\u03B8\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "periods = [\"yearone\", \"yeartwo\", \"yearthree\"]\n",
    "results = {beach:{} for beach in locs}\n",
    "\n",
    "\n",
    "\n",
    "def make_models_for_each_period(a_beachx, priors,ldng=ldng,results=results, periods=periods, period=3,  thetaunicode=thetaunicode, draws=2000):\n",
    "    \n",
    "    names = [f\"p-{a_beachx}\", f\"{thetaunicode}-{a_beachx}\"]         \n",
    "    index = len(priors)\n",
    "    \n",
    "    # all the data up to the period specified in period\n",
    "    # period is an array of index values that correspond\n",
    "    # to the sampling periods\n",
    "    prior_data = ldng.loc[(ldng.period.isin(priors))].groupby([\"loc_date\"]).fail.sum().to_numpy()\n",
    "    \n",
    "    # in some surveys both objects were identified\n",
    "    # anything greater than zero is considerered 1\n",
    "    prior_data[prior_data > 1] = 1\n",
    "    \n",
    "    # the number of times an FHP was found\n",
    "    FHP = sum(prior_data)\n",
    "    \n",
    "    # the number of samples\n",
    "    samps = len(prior_data)\n",
    "    \n",
    "    # the number times an FHP was not found\n",
    "    pk= samps-FHP        \n",
    "\n",
    "    observed = ldng.loc[(ldng.location.isin([a_beachx]))&(ldng.period.isin([period]))].groupby([\"loc_date\"]).fail.sum().values\n",
    "    # in some surveys both objects were identified\n",
    "    # anything greater than zero is considerered 1    \n",
    "    observed[observed > 1] = 1\n",
    "\n",
    "    # if there are observation of from the current\n",
    "    # sampling period use them\n",
    "    if len(observed):\n",
    "        pass \n",
    "\n",
    "    else:\n",
    "        # add one to the denominator and numerator\n",
    "        # that means that FHP = 1 and n samples = 2\n",
    "        # so that k, n-k = 1,1\n",
    "        observed = [0,1]\n",
    "    \n",
    "    model_name = f\"{a_beachx}-{periods[index]}\"\n",
    "\n",
    "    # returns an arviz data set\n",
    "    modelx = model_a_sampling_period(observed, FHP, pk, names=names, model_name=[model_name], draws=draws)\n",
    "\n",
    "    # update the dictionary       \n",
    "    results[a_beachx].update({\"model\":modelx[model_name]})\n",
    "\n",
    "def hdi_and_mcmc_summary(results, drop_these=[\"ess_bulk\", \"ess_tail\", \"mcse_mean\", \"mcse_sd\"]):\n",
    "    tabels = []\n",
    "    for a_model in results:\n",
    "        data = az.summary(results[a_model][\"model\"], fmt=\"long\")\n",
    "        tabels.append(data)\n",
    "    \n",
    "    table_data=pd.concat(tabels, axis=1)\n",
    "    table_data.drop(drop_these, inplace=True)\n",
    "    table_data.reset_index(inplace=True)\n",
    "    \n",
    "    return table_data\n",
    "\n",
    "\n",
    "\n",
    "def hdi_chart(table_data, columns=1, values=[0,2,3]):\n",
    "    hdis = table_data.columns[columns:]\n",
    "    hdi=table_data.loc[values, :][hdis]\n",
    "    \n",
    "    return hdi, hdis\n",
    "def chart_hdi_and_summary(hdi, hdis, figsize=(9,8), colorsx=[\"dodgerblue\",\"magenta\", \"chocolate\", \"lime\", \"slategrey\"]):\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    gs=GridSpec(11, 13, figure=fig)\n",
    "\n",
    "    axone = fig.add_subplot(gs[0:6,0:5])\n",
    "    axtwo = fig.add_subplot(gs[0:, 6:])\n",
    "\n",
    "    maxBeach = hdis[-1]\n",
    "\n",
    "    for i,beach in enumerate(hdis):\n",
    "        data = hdi.loc[:, hdis[i]].values\n",
    "        axone.hlines(y=i+.5, xmin=data[1], xmax=data[2], color=colorsx[i], linewidth=25,label=beach, alpha=.5)\n",
    "        axone.scatter(y=i+.5, x=data[0], c=\"black\", marker=\"x\", s=30, label=\"mean\")\n",
    "        if beach == maxBeach:\n",
    "            axone.vlines(x=data[0], ymin=0, ymax=len(hdis), linestyle=\"-.\")\n",
    "\n",
    "    axone.set_ylim(-.05, len(hdis))\n",
    "    handles, labels = axone.get_legend_handles_labels()\n",
    "\n",
    "    # remove the repetive labels\n",
    "    nh = handles[::2]\n",
    "    nl = labels[::2]\n",
    "\n",
    "    # put one back in\n",
    "    nl.append(labels[-1])\n",
    "    nh.append(handles[-1])\n",
    "\n",
    "    axone.legend(nh, nl, bbox_to_anchor=[0,-.15], loc=\"upper left\", labelspacing=2, borderpad=.4, frameon=False)\n",
    "\n",
    "    table_two = a_simple_formatted_table(axtwo,table_data.values,colLabels=table_data.columns, coded_labels=True, codes=[\"white\",*colorsx], colWidths=[.2, *[.16]*5], bbox=[0,0,1,1])\n",
    "    axtwo = remove_spines(axtwo)\n",
    "    axtwo = remove_ticks(axtwo)\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "ldng =  ldng.groupby([\"loc_date\",\"location\", \"period\"], as_index=False).agg({\"fail\":\"sum\"})\n",
    "# make a model fore each beach\n",
    "for each_beach in locs:\n",
    "    make_models_for_each_period(each_beach, [1,2], period=3, ldng=ldng, draws=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_data = hdi_and_mcmc_summary(results)\n",
    "hdi, hdis = hdi_chart(table_data)\n",
    "chart_hdi_and_summary(hdi, hdis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Above:__ the probable values of $\\theta_{FHP}$ for the survey locations in Geneva given all the previous data from the lake. __Left:__ the difference in means is less than 0.01 for any of the locations. __Right:__ villa-barton has the highest min and max HDI values.* \n",
    "\n",
    "The predicted value of theta given all the data from lake is very close to the lake value for all locations in Geneva. Locations on the right side could have a 1% advantage over the expected value of theta for the lake. \n",
    "\n",
    "If the survey results from Geneva are considered independently of the lake results the expected value of theta is less but the uncertainty increases. The range of the 94% HDI is atleast twice that if the lake data is considered. However, the locations on the right side still have the highest expected value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# the aggregated survey data\n",
    "# limit the prior to results in petite lac\n",
    "ldng = ldb[(ldb.code.isin(code))][[\"period\",\"location\", \"fail\", \"loc_date\"]].copy()\n",
    "ldng =  ldng[ldng.location.isin(plac_beaches)].groupby([\"loc_date\",\"location\", \"period\"], as_index=False).agg({\"fail\":\"sum\"})\n",
    "\n",
    "periods = [\"yearone\", \"yeartwo\", \"yearthree\"]\n",
    "results_nolake = {beach:{} for beach in locs}\n",
    "\n",
    "# make a model fore each beach\n",
    "for each_beach in locs:\n",
    "    make_models_for_each_period(each_beach, [1,2], results=results_nolake, ldng=ldng, draws=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_data = hdi_and_mcmc_summary(results_nolake)\n",
    "hdi, hdis = hdi_chart(table_data)\n",
    "\n",
    "chart_hdi_and_summary(hdi, hdis, figsize=(9,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Saint Sulpice__\n",
    "\n",
    "Students of Solid Waste engineering have been monitoring the beaches every year in november in St Sulpice since 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# the locations of interest\n",
    "locs = dfBeaches[(dfBeaches.city == \"Saint-Sulpice (VD)\")&(dfBeaches.water == \"l\")].index.unique()\n",
    "# thetaunicode = \"\\u03B8\"\n",
    "\n",
    "# code=[\"G96\", \"G144\"]\n",
    "\n",
    "# the aggregated survey data\n",
    "ldng = ldb[(ldb.code.isin(code))][[\"period\",\"location\", \"loc_date\",  \"fail\"]].copy()\n",
    "ldng =  ldng.groupby([\"loc_date\",\"location\", \"period\"], as_index=False).agg({\"fail\":\"sum\"})\n",
    "\n",
    "\n",
    "results_swe = {beach:{} for beach in locs}\n",
    "\n",
    "# make a model fore each beach\n",
    "for each_beach in locs:\n",
    "    make_models_for_each_period(each_beach, [1,2], results=results_swe, ldng=ldng, draws=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data = hdi_and_mcmc_summary(results_swe)\n",
    "hdi, hdis = hdi_chart(table_data)\n",
    "\n",
    "chart_hdi_and_summary(hdi, hdis, figsize=(9,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# the aggregated survey data\n",
    "# limit the prior to results in petite lac\n",
    "ldng = ldb[(ldb.code.isin(code))][[\"period\",\"location\", \"fail\", \"loc_date\"]].copy()\n",
    "ldng =  ldng[ldng.location.isin(glac_beaches)].groupby([\"loc_date\",\"location\", \"period\"], as_index=False).agg({\"fail\":\"sum\"})\n",
    "\n",
    "periods = [\"yearone\", \"yeartwo\", \"yearthree\"]\n",
    "results_nolake = {beach:{} for beach in locs}\n",
    "\n",
    "# make a model fore each beach\n",
    "for each_beach in locs:\n",
    "    make_models_for_each_period(each_beach, [1,2], results=results_nolake, ldng=ldng, draws=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data = hdi_and_mcmc_summary(results_nolake)\n",
    "hdi, hdis = hdi_chart(table_data)\n",
    "\n",
    "chart_hdi_and_summary(hdi, hdis, figsize=(9,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The field operations manager, Shannon Erismann, decided that the best chances of finding an FHP was at villa-barton. She considered the proximity to the train station and the time it takes to complete a survey as well as the likelihood of finding an FHP. The locations on the left bank of the lake,  (baby-plage, rocky-plage and baby-plage-ii) had only 3/14 positive samples in the most recent sampling period, well below the mean for the lake. FHP had been found at villa-barton and at jardin-botanique previously (4/15) and those locations had only been sampled twice in the previous period, (annex table 2a).\n",
    "\n",
    "The probabiltiy of finding an FHP in Geneva at a beach litter survey is between 20% and 40% which is less than the rest of the lake (32%-47%). The chances tend to be greater, +1% on the right bank if a suitable location to sample can be found. Whether or not this can be generalized to the \"Petit Lac\" was not explored. \n",
    "\n",
    "__Significance__\n",
    "\n",
    "The signifigance of these results depends on the question:\n",
    "\n",
    "> $H_{0}$ : The chance of finding at least one FHP on the beach is less than 20% at each survey.\n",
    "\n",
    "> $H_{1}$ : The chance of finding an FHP is greater than 20% at each survey\n",
    "\n",
    "The alternative hypothesis is the most likely, the minimum 3% HDI for the lake or Geneva is 24%, making values less than 24% highly unlikely. Locations that have values in these lower ranges may have practices or methods that help reduce the incidence of these objects. \n",
    "\n",
    "> $H_{0}$: The chance of finding an FHP is the same for all locations in Geneva\n",
    "\n",
    "> $H_{1}$: The chance of finding an FHP is different for all locations in Geneva\n",
    "\n",
    "The null hypothesis is the most likely. Each location has a unique mean but the 94% HDIs are very similar, suggesting that values above and below the mean are equally likely for each location. The small differences between the means can be exploited to improve chances of finding an FHP by 1% at best.\n",
    "\n",
    "__Covariates__\n",
    "\n",
    "Another measure of significance could be the value of covariates tested under the same conditions. Cotton swabs are associated with the same source as FHP but are found more frequently. {cite}`iqaasl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# the locations of interest\n",
    "locs = dfBeaches[(dfBeaches.city == \"Genève\")&(dfBeaches.water == \"l\")].index.unique()\n",
    "# thetaunicode = \"\\u03B8\"\n",
    "\n",
    "code = [\"G95\"]\n",
    "\n",
    "# the aggregated survey data\n",
    "ldng = ldb[(ldb.code.isin(code))][[\"period\",\"location\", \"fail\", \"loc_date\"]].copy()\n",
    "ldng =  ldng.groupby([\"loc_date\",\"location\", \"period\"], as_index=False).agg({\"fail\":\"sum\"})\n",
    "\n",
    "\n",
    "periods = [\"yearone\", \"yeartwo\", \"yearthree\"]\n",
    "results = {beach:{} for beach in locs}\n",
    "\n",
    "# make a model for each beach\n",
    "for each_beach in locs:\n",
    "    make_models_for_each_period(each_beach,  [1,2], ldng=ldng, results=results,draws=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "table_data = hdi_and_mcmc_summary(results)\n",
    "hdi, hdis = hdi_chart(table_data)\n",
    "\n",
    "chart_hdi_and_summary(hdi, hdis, figsize=(9,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Above:__ Figure 3, the probable values of $\\theta_{cotton-swabs}$ for the survey locations in Geneva. __Left:__ villa-barton has the highest average value by at least 1%. __Right:__ the 94% HDI ranges from 89% - 98%, approximately half the range for FHP.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When cotton swabs are considered the difference between villa-barton and the other locations is more evident. The probable range of values is between 85% and 98% a 13% difference compared to 20% for FHP. The elevated frequency of cotton swabs at the same place as FHP is consistent with previous results.\n",
    "\n",
    "If you absolutley need to find an FHP and are in Geneva then any information that can help you find one would be considered significant. If you are wondering about the occurence of other items and their rates of occurences then this is significant. If you are interested in building a recommender system that helps identify locations that accumulate or generate trash based on observations this may be significant, it is at least a starting point.\n",
    "\n",
    "__Ellas' adventure: the epilogue__\n",
    "\n",
    "Shannon gave a meeting place and time. A beach litter survey was conducted an FHP was found. ELla was on time for her meeting with the staff at Procter and Gamble. {cite}`20minutes`\n",
    "\n",
    "## Discussion\n",
    "\n",
    "The answer to Ellas question is at the heart of the multiple calls to reduce marine litter. It also has implications in the fields of marketing, life cycle assessment, envrionmental stewardship and governance. The EU released guidelines to calculate the median and threshold values of beach litter surveys conducted on European Seas. The results are intended to evaluate the _Good environmental standing_ of the locations surveyed according to the Marine Strategy Framework Directive (MSFD). The threhold value for _Good environmental standing_ was set at 20 objects of trash per 100 meters of beach.{cite}`eubaselines`\n",
    "\n",
    "This can be a tricky situation for stakeholders and regional adminsitrations. Consider a beach-litter survey as a detailed customer feedback form. At the same time that regional managers are trying to determine optimal resource allocation strategies, end users are planning the location of the next weekend getaway. In either case reliable data and transparent analysis methods are essential to making an informed decision.\n",
    "\n",
    "That these results correspond with the experience of the surveyors follows from the math. The derivation of the Beta-Binomial conjugate model is more complex than the its' proposition: `each survey within a region adds to the cumulative knowledge of that region and the locations that were surveyed`. This defines the benefit both of collecting data by hand and reporting the results as a likelihood or expectation of what a person may find. This often overlooked advantage imparts three critical pieces of information to the decision maker:\n",
    "\n",
    "> What the status was\n",
    "\n",
    "> What it would most likely be today\n",
    "\n",
    "> The source of the information\n",
    "\n",
    "If the survey results are considered reliable, stakeholders have a method to anticipate the user-experience and enact policies to improve that experience. This reduces the chances of miss communication and places the assessment of quality and satisfaction into the hands of the end-user. Many companies have used this formula to improve product quality and customer satisfaction. For producers of goods that appear on beach-litter surveys there is now a method to determine how likely a product will end up on a survey. This gives another metric to determine the accuracy of the LCA and improve product outcomes with respect to end of life cycle.\n",
    "\n",
    "The importance or signifigance of the results between individual beaches is allways a function of the number of samples at each beach. The assumption is that the probability of finding an object is about the same as the rest of the lake. Locations that have fewer samples in the most recent sampling period will tend to resemble the lake values more closely than locations that have more samples. The longer a location goes unsampled, the less weight its previous samples carry in the posterior distribution.\n",
    "\n",
    "Underpinning all of this potential is data reliability. There is tremendous pressure for organizations to _collect data_. A parallel structure to ensure good practices is yet to be proposed. If some portion of survey data variability can be attributed to the surveyor then there should be processes inplace to ensure that surveys are conducted in a consistent manner. \n",
    "\n",
    "__Next steps__\n",
    "\n",
    "The model proposed is scalable and reliable, the accuracy depends on the 94% HDI. Defining or locating more efficient methods to calculate the probability for all integer values would allow for the generalization of this process to all objects and locations. Understanding how the probability changes with respect to different independent variables can help stakeholders identify sources. The first steps were taken when $\\theta_{FHP}$ for all locations on lake were considered (annex Figure 1a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Below:__ Table 1a, the number of samples and the ratio of samples where at least one FHP was identified (rate).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fhg_summary[fhg_summary[\"n samples\"] > 2][[\"city\",\"location\", \"n samples\", \"s_pos\", \"rate\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Geneva there were 16 samples in period three and 12 samples in the first period. Using the same method as above the range of probable values in Geneva ranges from 0 to 66%. A probability of 0 is not realistic and 12/28 samples are almost four years old. The range of possible values reflects how uncertain any prediction may be using these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Table 2a:__ locations, number of samples and number of samples with at least one FHP in Geneva*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "glocs_summary = geneva.groupby(['location', 'period'], as_index=False).agg({'loc_date':'nunique', 'fail':'sum'})\n",
    "glocs_summary.rename(columns={'loc_date':'samples','fail':'FHP', 'location':'locations'}, inplace=True)\n",
    "glocs_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Villa-barton had 0/9 positive samples in the first period and 1/2 in the third period. Jardin-botanique had 2/3 in period one but has not been sampled since. To account for the results from prior  sampling periods bayesian inference will be used to estmate the location in Geneva that may have the highest value of $\\theta$ with respect to the other locations given the previous survey results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the locations of interest\n",
    "locs = dfBeaches[(dfBeaches.water_name_slug == \"lac-leman\")&(dfBeaches.water == \"l\")].index.unique()\n",
    "thetaunicode = \"\\u03B8\"\n",
    "\n",
    "# the aggregated survey data\n",
    "code = [\"G96\", \"G144\"]\n",
    "ldng = ldb[(ldb.code.isin(code))][[\"period\",\"location\",\"loc_date\", \"p/100m\"]].copy()\n",
    "\n",
    "ldng =  ldng.groupby([\"loc_date\",\"location\", \"period\"], as_index=False).agg({\"p/100m\":\"sum\"})\n",
    "ldng[\"fail\"] = ldng[\"p/100m\"] > 0\n",
    "\n",
    "\n",
    "periods = [\"yearone\", \"yeartwo\", \"yearthree\"]\n",
    "results = {beach:{} for beach in locs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "for each_beach in locs:\n",
    "    make_models_for_each_period(each_beach,  [1,2], ldng=ldng, results=results,draws=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables = []\n",
    "# for a_model in results:\n",
    "#     data = az.summary(results[a_model][\"model\"], fmt=\"long\")\n",
    "#     tables.append(data)\n",
    "\n",
    "# table_data = pd.concat(tables, axis=1)\n",
    "# table_data.drop([\"ess_bulk\", \"ess_tail\", \"mcse_mean\", \"mcse_sd\"], inplace=True)\n",
    "# table_data.reset_index(inplace=True)\n",
    "\n",
    "# colorsx = plt.cm.get_cmap(\"hsv\", len(locs))\n",
    "\n",
    "# # the HDI per location\n",
    "# hdis = table_data.columns[1:]\n",
    "# hdi = table_data.loc[[0,2,3],:][hdis]\n",
    "# hdi.loc[:, hdis[0]].values\n",
    "# hdi = hdi.sort_values(by=0, axis=1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data = hdi_and_mcmc_summary(results)\n",
    "hdi, hdis = hdi_chart(table_data)\n",
    "colorsx = plt.cm.get_cmap(\"hsv\", len(locs))\n",
    "\n",
    "# chart_hdi_and_summary(hdi, hdis, figsize=(9,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, len(locs)*.8))\n",
    "\n",
    "axone= ax\n",
    "\n",
    "colorsx = plt.cm.get_cmap(\"hsv\", len(locs))\n",
    "\n",
    "maxBeach = hdi.columns[0]\n",
    "\n",
    "for i,beach in enumerate(hdi.columns):\n",
    "    data = hdi.loc[:, beach].values\n",
    "    axone.hlines(y=i+.5, xmin=data[1], xmax=data[2], color=colorsx(i), linewidth=25,label=beach, alpha=.5)\n",
    "    axone.scatter(y=i+.5, x=data[0], c=\"black\", marker=\"x\", s=30, label=\"mean\")\n",
    "    if beach == maxBeach:\n",
    "        axone.vlines(x=data[0], ymin=0, ymax=len(hdis), linestyle=\"-.\")\n",
    "\n",
    "axone.set_ylim(-.05, len(hdis))\n",
    "handles, labels = axone.get_legend_handles_labels()\n",
    "\n",
    "# remove the repetive labels\n",
    "nh = handles[::2]\n",
    "nl = labels[::2]\n",
    "\n",
    "nh = nh[::-1]\n",
    "nl = nl[::-1]\n",
    "\n",
    "# put one back in\n",
    "nl.append(labels[-1])\n",
    "nh.append(handles[-1])\n",
    "\n",
    "axone.legend(nh, nl, bbox_to_anchor=[1,1], loc=\"upper left\", labelspacing=2, borderpad=.4, frameon=False)\n",
    "\n",
    "# table_two = a_simple_formatted_table(axtwo,table_data.values,colLabels=table_data.columns, coded_labels=True, codes=[\"white\",*colorsx], colWidths=[.2, *[.16]*5], bbox=[0,0,1,1])\n",
    "# axtwo = remove_spines(axtwo)\n",
    "# axtwo = remove_ticks(axtwo)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_lac = dfBeaches[dfBeaches.city.isin(glac)].copy()\n",
    "g_lac[\"region\"] = \"grand-lac\"\n",
    "p_lac = dfBeaches[dfBeaches.city.isin(plac)].copy()\n",
    "p_lac[\"region\"] = \"petit-lac\"\n",
    "\n",
    "mapvals = pd.concat([g_lac, p_lac])\n",
    "mapvals.to_csv(\"resources/mapvals.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "author = \"roger@hammerdirt.ch\"\n",
    "my_message = \"\\u2764\\ufe0f what you do.\"\n",
    "md(F\"\"\"\n",
    "<br></br>\n",
    "**Hammerdirt everyday.**<br>\n",
    "\n",
    ">{my_message}<br>\n",
    "\n",
    "*{author}* pushed the run button.<br>\n",
    "This document originates from https://github.com/hammerdirt-analyst/ all copyrights apply.<br>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
